\documentclass[a4paper]{article}

\usepackage{afterpage}

\usepackage{authblk}
\renewcommand\Affilfont{\itshape\small}

%%% Кодировки и шрифты %%%
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian,english]{babel}
\usepackage{amssymb,amsfonts,amsmath,amsthm,amscd,mathtext}

\usepackage{cmap}   % чтобы работал поиск по pdf
\usepackage{graphicx}
\pdfcompresslevel=9

%% красная строка
\usepackage{indentfirst}
\setlength{\parindent}{5.5ex}   % 5 символов

%% интерлиньяж
\linespread{1.1}                   

%%% Общее форматирование
\usepackage[singlelinecheck=off,center]{caption} % Многострочные подписи
\usepackage{soul} % Поддержка переносоустойчивых подчёркиваний и зачёркиваний

%%% Гиперссылки %%%
\usepackage[plainpages=false,pdfpagelabels=false]{hyperref}

\usepackage{geometry}
\geometry{a4paper,top=2cm,bottom=2cm,left=2cm,right=1cm}

%%% Выравнивание и переносы %%%
\sloppy
\clubpenalty=10000
\widowpenalty=10000

%%% biblatex
\usepackage{csquotes} % recommended by pdflatex
\usepackage[%
    style = authoryear-comp,
    dashed = false,
    citestyle = numeric,
    firstinits = true,        
    sorting = anyt,
    autolang = other,
    language = auto,
    natbib = true,
    backend = biber,
    defernumbers = true,
    sortlocale = ru
]{biblatex}
\addbibresource{bibliography/bibliography.bib}

\defbibenvironment{bibliography}
{\enumerate{}
{\setlength{\leftmargin}{\bibhang}%
\setlength{\itemindent}{-\leftmargin}%
\setlength{\itemsep}{\bibitemsep}%
\setlength{\parsep}{\bibparsep}}}
{\endenumerate}
{\item}


\usepackage{hyperref}
\hypersetup{%
    colorlinks = true,
    hidelinks = false
}

%% graphics
\graphicspath{{images/}}

%% math environments
\newtheoremstyle{example-style}% name
{5pt}  % space above 
{5pt}  % space below 
{}     % body font
{\parindent}  % indent 
{\bfseries}  % theorem head font
{.}  % punctuation after theorem head
{.5em}  % space after theorem head 
{}  % theorem head spec (can be left empty, meaning 'normal')

\theoremstyle{example-style}
\newtheorem{example}{Example}


%% Nat­u­ral de­duc­tion proofs in styles used by Jaśkowski and Kal­ish and Mon­tague
\usepackage{natded}

%% Gentzen style natural deduction proof trees
\usepackage{bussproofs}
\usepackage{latexsym}

%%% linguistic packages
\usepackage{tikz-qtree,tikz-qtree-compat}   % regular trees (e.g. GB style)
\usepackage{tikz-dependency}                % dependency trees (bracket style)
\usetikzlibrary{matrix,arrows}              % for commutative diagrams

\usepackage{gb4e}  % numbered lists for linguistic examples (IMPORTANT: If you use gb4e package, let it be the last \usepackage call in the document's preamble. Otherwise you may get exceeded parameter stack size error.)


\begin{document}

\title{Syntactic Annotation by Interactive Proof Search}
\author[1]{Konstantin Sokolov}
\author[2]{Dimitri Timofeev}
\author[3]{Yuri Kizhaev}
\affil[1]{Department of Mathematical Linguistics, St.~Petersburg State University, St.~Petersburg, Russia}
\affil[2]{Institute of Computer Science and Technology, Peter the Great St.~Petersburg Polytechnic University, St.~Petersburg, Russia}
\affil[3]{Department of Algebra and Number Theory, St.~Petersburg State University, St.~Petersburg, Russia}
\date{}

\maketitle

\section{Introduction}

The purpose of this paper is to introduce a method of syntactic annotation in the categorial grammar formalism, that fully utilizes its logical nature to automate routine tasks and ensure high quality of the resulting corpus. Categorial grammar, being one of the oldest formal approaches to syntax, has achieved significant adoption in the recent years. It is an attractive formalism for creation of syntactically annotated corpora, with CCGbank \parencite{hockenmaier2007ccgbank} being the notable example of a widely useful linguistic resource of that kind. A method adopted by its creators consisted in semi-automatic conversion of a corpus of another format (namely, constituency structures). Another commonly considered possibility is some form of manual annotation, possibly using a third party parser to obtain preliminary parses prior to manual error correction. It is also possible to bootstrap a corpus along with a parser by training, parsing and performing error correction iteratively. Whatever the method, creation of a corpus is extremely time consuming and demands high qualification from annotators. Powerful and flexible tools are of great importance for anyone involved in such an effort.

Our aim is to create a freely available corpus of Russian with syntactic annotation in a categorial grammar formalism (analoguously to CCGbank for English) which can be used as a basis for statistically trained semantic parser for Russian (following \parencite{clark2007wide} and works in syntax-semantics interface for CCG  by Clark, Bos, Grefenstette, Sadrzadeh and others). Some special considerations are to be made when applying categorial grammar to Russian. For historical reasons, the Russian community has a strong bias towards dependency grammar \parencite{toldova2012nlp}. There are no freely available Russian corpora with constituency structure annotation, therefore we cannot use an approach of \parencite{hockenmaier2007ccgbank} directly. An existing SynTagRus corpus \parencite{boguslavsky2002development} is non-free and dependency-based, which is contrary to our goals and problematic for the  approach used for creation of CCGbank, since it would require an extra step of converting to phrase structure formalism with large amount of manual error correction. In this paper we propose another method of annotation based on interactive proof search, oriented towards the logical branch of categorial grammar. We adopt many ideas of the French school of logic and automated reasoning (Moot, Retor\'{e}, Coquand, Huet, Girard). A morphologically annotated corpus is required as input and we currently use a fully disambiguated subcorpus from OpenCorpora \parencite{granovsky2010opencorpora} as a starting point. However, our method applies to any morphologically annotated corpus. We expect that our approach can significantly speed up the annotation process due to automation of routine tasks and quality assurance.
     

\section{Related Work}

The most prominent logical approach to syntax, \textit{categorial grammar}, stems from works by Ajdukiewicz and Bar-Hillel \parencite{ajdukiewicz1935,bar1953quasi}. In the late 50s Lambek proposed a formulation of categorial grammar as a deductive system \parencite{lambek1958mathematics,lambek1961calculus}, which initiated a whole new branch of logical syntax \parencite{moot2012logic}. Another form of categorial grammar, Combinatory Categorial Grammar, proposed by Steedman \parencite{steedman2000syntactic}, builds on ideas from combinatory logic and has developed into a highly successful non-transformational lexicalized syntactic formalism with many applications. Currently, many flavours of categorial grammar exist, including Type Logical Grammar \parencite{morrill1994type} and Abstract Categorial Grammar \parencite{degroot2001towards}, to name a few. Development of multimodal extensions for both logical and combinatorial versions allowed for good control of expressivity and helped overcome some known problems like spurious ambiguity. Valuable tools, inculding a number of wide-coverage statistical parsers \parencite{curran2007linguistically,white2008open} were developed. Among the positive aspects of the approach is its clear connection with various theories of semantic modeling, both logical \parencite{steedman2000syntactic,bos2008wide,baldridge2002coupling} and distributional \parencite{maillard2014type}. Capability of expressing long-range dependencies and word order alternations makes it promising as a formal basis for modeling syntax of free word order languages like Russian. A related branch of logical syntax, known as \textit{parsing as deduction}, is a set of techniques which proved useful for analysis of formal properties of parsing algorithms and development of parser technology \parencite{pereira1983parsing,kallmeyer2010parsing}.

Automated reasoning is the use of computer systems to perform logical inference. There are two approaches, automated theorem proving and interactive proof search. Fully automated inference has theoretical limitations and even if deduction is possible in principle, the space and time considerations can render it unpractical. Proof assistants are more flexible since they do not try to do the whole inference automatically, leaving some important decisions to the user. The basis of proof assistant technology is type theory (dependent types \parencite{martinloef1984}, calculus of constructions \parencite{coquand1988calculus}, etc.) and the principle of propositions as types. Coq, Nuprl, Isabelle/HOL together with dependently typed programming languages like Agda and Idris are among the proof assistants that have the greastest adoption. Some of the systems support extensions with new decision procedures written in either the implementation language of the tool (such as OCaml for Coq, Standard ML for Isabelle/HOL) or a  domain-specific language (Ltac for Coq).

Due to the Curry-Howard correspondence, the deductive formulation of syntactic procedures can be given a computational implementation. Multiple attempts of applying automated reasoning technology to syntax and semantics of natural language are known. Among these proposals are Grail3, an interactive prover based on proof nets for non-associative Lambek calculus \parencite{moot2002proof}, Icharate library implemented in Coq \parencite{anoun2007approche}, a Prolog-based prover CatLog for Type-Logical Grammar \parencite{morrill2012catlog}. A series of works on Natural Language Inference with implementations in Coq \parencite{chatzikyriakidis2014natural}, although devoted primarily to semantics, are important examples of usefulness of interactive reasoning for dealing with intricacies of natural language.


\section{Our Approach}

Final artifacts of the project are a database containing assignments of syntactic categories to word forms (many to many relation), canonical derivations (parses) for each sentence and scripts that a proof assistant should use to verify these derivations. A possible byproduct is a library of tactics, i.\,e. specialized subroutines that formalize some high-level syntactic constructs and were used to produce the parses. 

The proposed method of annotation requires a preliminary step of initial mapping of existing morphological tags to syntactic categories. The tagset used in the OpenCorpora project consists of 113 morphological tags and a number of auxilary tags like PNCT for punctuation, NUMB for numbers, etc. It is straightforward to assign the most simple syntactic categories to word forms based on their morphological features like part of speech, case, transitivity etc.

\begin{table}[ht]
\centering
\caption{Initial mapping of grammemes to syntactic categories}
\label{my-label}
\begin{tabular}{lll}
NOUN, nomn & n         & \textbf{школа}                              \\
NOUN, accs & n         & \textbf{язык}                               \\
NOUN, gent & n\textbackslash n      & школа \textbf{злословия}       \\
VERB, tran & (s\textbackslash n)/s & \textbf{учит} прикусить язык    \\
INFN       & s/n       & \textbf{прикусить}                          \\
PNCT       & n/n       & \textbf{\textless\textless} школа           \\
PNCT       & n\textbackslash n      & злословия \textbf{\textgreater\textgreater} 
\end{tabular}
\end{table}    
  
We adopt a ``layered'' approach of multimodal extensions of categorial grammar \parencite{morrill1994type,baldridge2003multi}, which can be seen as a requirement to use the mininimal subset of inference rules where possible. This ensures that we obtain the simplest parses first and avoid use of the more expressive portions of the formalism unless strictly required. Another benefit of this strategy is the possibility to introduce a measure of parse complexity based on the rules used for deduction and the complexity of syntactic categories that were chosen for the word forms. Having an initial assignment, a limited set of parses is created automatically to serve as a basis for an iterative annotation process, consisting of the following steps. First, pick a partially parsed sentence, i.\,e. a sentence where some words lack an assignment of syntactic category which can license an acceptable parse. To fill in the gaps, a deduction is built interactively with assistance from the system, consisting of instant evaluation of variants and performing trivial steps of deduction in a fully automated manner. When the deduction is complete, the missing assignments are stored in the database. At the next step, an automatic analysis of the corpus is preformed to determine what new parses are made possible by the addition. The system may automatically produce the new parses, compute their structural complexity and propose an ordered list of parses for review. 

A standard feature of proof assistant technology is the possibility to extract large portions of typical deductions into tactics. In our setting tactics can represent special syntactic constructs that frequently appear in the corpus and can be applied at a certain point in the deduction process. For instance, a passive voice can be easily determined by an annotator on the basis of the meaning of a sentence, but it might require considerable effort to provide a correct formal encoding, therefore it is desirable to be able to reuse the applied strategy where applicable. Besides having access to morphological information in the corpus and the previous (partial) parses, tactics can use another tactics and encode high level syntactic information, which makes it a very powerful tool. Moreover, if for some reason a syntactic construct need be given a different treatment, it is much easier to make changes to the corresponding tactic than to manually fix all incorrect parses, as the tactics are applied automatically and the updated parses are produced without manual intervention.
  

\section{Discussion}

Whereas OpenCorpora is a community effort, in our approach crowdsourcing is not applicable. However, the requirements for the annotators are not too high, since in our approach there will be much less decisions to make as compared to the traditional approach to annotation. A decent library of tactics encoding standard syntactic constructs can help a lot. It is yet to be determined as to what syntactic constructs and annotation strategies can be automated by scripting.

To assess enhancement of the annotation process a number of statistics has to be collected. An estimation of the number of syntactic categories in the final database is important (cf. ca. 1200 for CCGBank). Also estimates of the number of possible parses of a sentence (real and spurious ambiguity), including variants of a formalism of different expressivity (like associative vs. non-associative Lambek calculus), have to be made. It should be noted, however, that we are interested in obtaining a single canonical parse in a text and not all possible parses, therefore ambiguity is much less problematic for an annotation tool than for a parser.
  
Another promising area of application of logic-based syntactic annotation is the possibility to analyze syntactic structure from the standpoint of expressivity of a formalism that validates a particular parse. It is still an open area of investigation as to what expressivity is required for various subsystems of natural language and sound empirical data can help get insights into these matters.



\printbibliography[resetnumbers=true]

\end{document}
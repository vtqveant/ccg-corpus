\documentclass[a4paper]{article}

\usepackage{afterpage}

\usepackage{authblk}
\renewcommand\Affilfont{\itshape\small}

%%% Кодировки и шрифты %%%
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian,english]{babel}
\usepackage{amssymb,amsfonts,amsmath,amsthm,amscd,mathtext}

\usepackage{cmap}   % чтобы работал поиск по pdf
\usepackage{graphicx}
\pdfcompresslevel=9

%% красная строка
\usepackage{indentfirst}
\setlength{\parindent}{5.5ex}   % 5 символов

%% интерлиньяж
\linespread{1.1}                   

%%% Общее форматирование
\usepackage[singlelinecheck=off,center]{caption} % Многострочные подписи
\usepackage{soul} % Поддержка переносоустойчивых подчёркиваний и зачёркиваний

%%% Гиперссылки %%%
\usepackage[plainpages=false,pdfpagelabels=false]{hyperref}

\usepackage{geometry}
\geometry{a4paper,top=2cm,bottom=2cm,left=2cm,right=1cm}

%%% Выравнивание и переносы %%%
\sloppy
\clubpenalty=10000
\widowpenalty=10000

%%% biblatex
\usepackage{csquotes} % recommended by pdflatex
\usepackage[%
    style = numeric,
    singletitle = false,
    autolang = other,
    language = auto,
    backend = biber,
    defernumbers = true,
    sortlocale = ru
]{biblatex}
\addbibresource{bibliography/bibliography.bib}

\defbibenvironment{bibliography}
{\enumerate{}
{\setlength{\leftmargin}{\bibhang}%
\setlength{\itemindent}{-\leftmargin}%
\setlength{\itemsep}{\bibitemsep}%
\setlength{\parsep}{\bibparsep}}}
{\endenumerate}
{\item}


\usepackage{hyperref}
\hypersetup{%
    colorlinks = true,
    hidelinks = false
}

%% graphics
\graphicspath{{images/}}

%% math environments
\newtheoremstyle{example-style}% name
{5pt}  % space above 
{5pt}  % space below 
{}     % body font
{\parindent}  % indent 
{\bfseries}  % theorem head font
{.}  % punctuation after theorem head
{.5em}  % space after theorem head 
{}  % theorem head spec (can be left empty, meaning 'normal')

\theoremstyle{example-style}
\newtheorem{example}{Example}


%% Nat­u­ral de­duc­tion proofs in styles used by Jaśkowski and Kal­ish and Mon­tague
\usepackage{natded}

%% Gentzen style natural deduction proof trees
\usepackage{bussproofs}
\usepackage{latexsym}

%%% linguistic packages
\usepackage{tikz-qtree,tikz-qtree-compat}   % regular trees (e.g. GB style)
\usepackage{tikz-dependency}                % dependency trees (bracket style)
\usetikzlibrary{matrix,arrows}              % for commutative diagrams

\usepackage{gb4e}  % numbered lists for linguistic examples (IMPORTANT: If you use gb4e package, let it be the last \usepackage call in the document's preamble. Otherwise you may get exceeded parameter stack size error.)


\begin{document}

\title{Syntactic Annotation by Interactive Proof Search}
\author[1]{Konstantin Sokolov}
\author[2]{Dimitri Timofeev}
\author[3]{Yury Kizhaev}
\affil[1]{Department of Mathematical Linguistics, St.~Petersburg State University, St.~Petersburg, Russia}
\affil[2]{Institute of Computer Science and Technology, Peter the Great St.~Petersburg Polytechnic University, St.~Petersburg, Russia}
\affil[3]{JetBrains Research, St.~Petersburg, Russia}
\date{}

\maketitle

\section{Introduction}

Categorial Grammar (CG), being one of the oldest formal approaches to syntax, has achieved significant adoption in the recent years. Development of multimodal extensions for both its logical version (Lambek calculus) and its combinatorial version (Combinatory Categorial Grammar) allowed for good control of expressivity and helped overcome some known problems like spurious ambiguity. Valuable tools, inculding a number of wide-coverage statistical parsers \parencite{curran2007linguistically,white2008open} were developed. Among the positive aspects of the approach is its clear connection with various theories of semantic modeling, both logical (Montague Grammar \parencite{steedman2000syntactic}, Discourse Representation Theory \parencite{bos2008wide}, Hybrid Logic Dependency Semantics \parencite{baldridge2002coupling}) and distributional \parencite{maillard2014type}. Capability of expressing distant dependencies and word order alternations by type shifting makes it promising as a formal basis for modeling syntax of free word order languages like Russian.

Variants of CG seem to be an attractive formalism for syntactic annotation of corpora, with CCGBank \parencite{hockenmaier2007ccgbank} being the notable example of a widely useful linguistic resource of that kind. An approach adopted by Hockenmaier and Steedman consisted in semi-automatic conversion of a corpus of another format (namely, constituency structures). Another commonly considered possibilities are manual annotation, possibly using a third party parser to obtain preliminary parses prior to manual error correction. It is also possible to bootstrap a corpus along with a parser by training, parsing and performing error correction iteratively.

Our aim is to create a freely available corpus of Russian with syntactic annotation in a formalism related to categorial grammar (analoguously to CCGBank for English) which can be used as a basis for statistically trained semantic parser for Russian (following \parencite{clark2007wide} and works in syntax-semantics interface for CCG (Clark, Bos, Grefenstette, Sadrzadeh and others). Some special considerations are to be made when applying CG to Russian. For historical reasons, russian NLP community has a strong bias towards dependency grammar \parencite{toldova2012nlp}. There are no freely available Russian corpora with phrase structure annotation, therefore we cannot use an approach of \parencite{hockenmaier2007ccgbank} directly. An existing SynTagRus corpus \parencite{boguslavsky2002development} is non-free and dependency-based, which is contrary to our goals and problematic for our approach. It would require extra effort to convert it to phrase structure formalism with large amount of manual error correction and quality check, which makes an idea of applying the Hockenmaier and Steedman's method less attractive.
 
In this paper we propose another method of annotation based on interactive proof search, oriented towards the logical branch of categorial grammar and adopting many ideas of the French school of logic and automated reasoning (Moot, Retor\'{e}, Coquand, Huet, Girard). We require as input a morphologically annotated corpus and currently use a fully disambiguated subcorpus from OpenCorpora \parencite{granovsky2010opencorpora} as a starting point. However, our method applies to any morphologically annotated corpus. We expect that our approach can significantly speed up the annotation process due to automatization of routine tasks and quality assurance.
     

\section{Related Work}

Logical approach to syntax
 \begin{itemize}
 	\item parsing as deduction \parencite{pereira1983parsing,kallmeyer2010parsing}
 	\item Lambek calculus \parencite{lambek1958mathematics,lambek1961calculus,konig1989parsing}
 	\item combinatory logic as a basis for extensions of categorial grammar (Steedman ?)
 \end{itemize}
 
Lexical semantics based of type-theory (starting with Ranta 1994) builds on earlier proposals by Montague and allows for a connection with the compositional treatment of semantics of the sentence. Later proposals by Cooper, Asher, Luo, Retor\'{e} and others are non-trivial extensions of the initial ideas which build on various type systems. Due to Curry-Howard correspondence these proposals can be given a computational implementation. This initiated multiple attempts to apply proof assistant technology to syntax and semantics of natural language. Among these proposals are Grail3, an interactive prover based on proof nets for non-associative Lambek calculus \parencite{moot2002proof}, Icharate library implemented in Coq \parencite{anoun2007approche}, a Prolog-based prover CatLog for Type-Logical Grammar \parencite{morrill2012catlog}, a series of works in Natural Language Inference with implementations in Coq \parencite{chatzikyriakidis2014natural}.
 
Some remarks about OpenCorpora and their morphological information (tagset, builds on Zaliznyak's dictionary).


\section{Our Approach}

As CG is lexicalized, the final artifact is a database containing assignments of syntactic categories to word forms (many to many relation).

  * Initial mapping of existing morphological tags to syntactic categories; \\
  
  * layered approach (multimodal grammars) -- use the mininimal subset where possible; manual intervention when the inference process is stuck; a measure of parse complexity (which rules were used in the deduction, how complex are the syntactic categories used) \\
    
  * addition of a syntactic category for a word form results in building a set of possible parses for sentences containing that form; these parses can be built fully automatically, a user is required only for an audit thereof. In many cases even the selection from the possible parses can be automated (heuristic: the simplest parse is better, parse complexity is computed automatically) \\
  
  * thus the proposed procedure of annotation. The preliminary step consists of assigning basic syntactic categories to words based on their morphological annotation (e.g. their POS tags map straightforwardly to the basic syntactic categories, e.g. NOUN maps to np, ADJV to np/np etc.) This way an initial limited set of parses is created, which contains only the simplest possible deductions. Next we apply iteratively the following procedures:
  \begin{itemize}
  	\item we pick a partially parsed sentence (that for which there is only one assignment of a syntactic category to a word form is missing), a deduction is built interactively with the help from the system; when the deduction is complete, the missing syntactic category is added to the DB
  	\item when a new synt. category is added, the system automatically builds parses which are made possible by the addition; the user checks the parses and accepts the correct ones; to the new cycle
  \end{itemize} 
  
  * standard syntactic constructions (coordination, government relation, distant dependencies, etc.) can be extracted to tactics 
    
Examples:

\begin{table}[]
\centering
\caption{Initial mapping of grammemes to syntactic categories}
\label{my-label}
\begin{tabular}{lll}
NOUN, nomn & n         & \textbf{школа}                              \\
NOUN, accs & n         & \textbf{язык}                               \\
NOUN, gent & n\textbackslash n      & школа \textbf{злословия}       \\
VERB, tran & (s\textbackslash n)/s & \textbf{учит} прикусить язык    \\
INFN       & s/n       & \textbf{прикусить}                          \\
PNCT       & n/n       & \textbf{\textless\textless} школа           \\
PNCT       & n\textbackslash n      & злословия \textbf{\textgreater\textgreater} 
\end{tabular}
\end{table}    
    
Example 2. \textless\textless Школа злословия \textgreater\textgreater учит прикусить язык
This sentence has 18 parses with the above mapping.


\section{Discussion}

  * estimation of the number of syntactic categories (cf. CCGBank ca. 1200), subcategorization \\
  
  * estimation of the size of the sets of automatically parsed sentences after a form/category is added to DB (apparently, word frequency is important, but also sentence complexity) \\
  
  * estimation of the number of possible parses of a sentence (real/spurious ambiguity), also for variants of formalism with different expressivity (like associative vs. non-associative Lambek calculus). However, we are interested in obtaining a single canonical parse in a text, not all possible parses (which is the task for a parser, not for a corpus annotation tool) \\
  
  * proof tactics -- what syntactic constructions and annotation strategies can be automated by scripting (cf. tactics in Coq) \\
  
  * crowdsourcing is not applicable in our approach \\
  
  * what are the requirements for the annotators? Supposedly, not too high, since in our approach there will be much less decisions  to make as compared to the traditional approach, which would require lower qualification in the area of theoretical syntax. Also, a decent library of tactics can help a lot. \\
  
  * estimation of the speed of annotation process. Tentatively it should be possible to annotate a million word corpus in one year.\\
  
  * a library of tactics as a syntactic theory \\
  
  * a syntactic formalism (e.g. Lambek calculus with product, LC non-associative, with different sets of deduction rules) as a theory implemented in a proof assistant. The possibility of validation of parses against various formal theories of syntax, the possibility to analyze syntactic structure from the standpoint of expressivity of a formalism that validates a parse (estimation of syntactic complexity as deducibility in the particular formal theory); such information in a corpus is valuable data and it is proved infromation (cf. the question whether natural language is context-free, which was a long-standing problem, and the notion of mild context sensitivity by A. Joshi, such data can help get insights about these matters).

\section{Conclusion}

\printbibliography[resetnumbers=true]

\end{document}